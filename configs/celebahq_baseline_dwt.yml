define: &_img_size 256
define: &_patch_size 4
define: &_approx_img_size 128

pretrained_model_ckpt: ""
batch_size: 16
lr: 5.0e-5 # 2.0e-4
adam_betas: [0.99, 0.99]
weight_decay: 0.03
results_folder: ""

ema_decay: 0.999
ema_update_every: 10
beta_schedule: cosine

shift: false
loss_weight: "p2"
p2_loss_weight_gamma: 0

num_workers: 5

dataset:
  PATH: ""  
  NAME: celebahq
  TASK: uncond

  USE_DWT: true
  DWT_LEVEL: 1

  NORMALIZATION: true # not working when using maskdwt model

  MASK_TYPE: "block"
  MASK_CROP_SIZE: -1
  MASK_RATIO: [0.0, 0.0] # mask ratio for approximation and detail respectively
  MASK_BLOCK_SIZE: 4

  IMG_SIZE: *_img_size
  SHORT_SIDE_SIZE: 256
  MEAN: [0.5,0.5,0.5]
  STD: [0.5,0.5,0.5]
  hflip_prob: 0.5

network:
  name : 'maskdwt'
  img_size : *_approx_img_size
  patch_size : *_patch_size
  in_chans : 12
  data_chans: 3
  level: 1 # dwt level
  scale: 1 # same patch size

  encoder_embed_dim : 1024
  encoder_depth : 16
  encoder_heads : 16

  decoder_depth: 0

  mlp_ratio : 4
  qkv_bias : True

  num_classes: -1

  use_final_conv: False
  mlp_time_embed : False
  use_checkpoint : False
